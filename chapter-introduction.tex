\chapter{Introduction}
\label{chapter-introduction}

% The topic of this thesis is the specification of {\it evolving systems}

% The {\it lingua franca} of research in programming languages and
% logics is the {\it inductive definition}. Type systems are defined in
% terms of inductive definitions like $\Gamma \vdash e : \tau$
% (within the typing context $\Gamma$, the term $e$ has type $\tau$).
% % \[
% % \infer
% % {\Gamma \vdash x : \tau \mathstrut}
% % {x{:}\tau \in \Gamma \mathstrut}
% % \quad
% % \infer
% % {\Gamma \vdash {\sf z} : {\sf nat} \mathstrut}
% % {}
% % \quad
% % \infer
% % {\Gamma \vdash {\sf s}\,e : {\sf nat} \mathstrut}
% % {\Gamma \vdash e_1 : {\sf nat}
% %  &
% %  \Gamma \vdash e_2 : {\sf nat}}
% % \]
% Operational semantics are also defined in terms of inductive
% definitions: a {\it small-step} semantics has the form $e \mapsto e'$
% (the expression or machine state $e$ can transition to $e'$), and a
% {\it big-step} semantics has the form $e \Downarrow v$ (the expression
% or machine state $e$ can ultimately produce the terminal machine state
% $v$). 

% Proof assistants -- computer programs that help programming language
% researchers specify systems, explore their behavior, and prove
% properties of their behavior -- obviously must therefore be able to
% talk about inductive definitions. The most common way proof assistants
% do this is by directly incorporating a notion of inductive definition
% into their framework. Coq \cite{}, Agda \cite{}, ATS \cite{},
% Isabelle/HOL \cite{}, Matita \cite{}, and Abella \cite{} all work this
% way; inductive types are introduced

%  which covers all the theorem provers used to prove the POPLMark
% challenge except for Twelf.

% used in these domains therefore universally
% include a notion of {\it inductive definition} as the primary form of

\section{Overview}
{\it Photometric stereo}, the de-facto term used for extracting shape from multiple directionally lit images, is almost as old as computer vision itself. At it's core, it is fundamentally the solution to a linear system that relies on the fact that exitant radiance from a lambertian surface patch is proportional to the dot product of incident light direction and the path normal. When it came out, this algorithm was extremely powerful. For most objects, it could estimate a per-pixel normal, leading to very accurate shape reconstructions.\\
It does, however, make several assumptions:
\begin{enumerate}
    \item Distant lights (Directional lighting)
    \item Convex shape (No multiple bounces/interreflections)
    \item Lambertian BSDF ($\mathbf{n}.\mathbf{l}$ shading)
\end{enumerate}

These assumptions do not sit very well with real world objects. For instance, the lambertian BSDF assumption breaks down for even the most diffuse objects, because of a grazing-angle \textit{Fresnel} component. This is the same phenomenon where seemingly rough surfaces start to behave like mirrors when viewed at an angle close to parallel with the surface.

The assumption of convexity also does not hold true for most objects, though its effects are harder to pin down accurately. Interreflections occur whenever two patches of an object face each other ($\mathbf{n_1}.\mathbf{n_2} > 0$. This means they are most pronounced near sharp inward edges. There also extremely concave surfaces like bowls and mugs that are rarely, if ever, used in photometric stereo papers because of extreme inter-reflections.

The final assumption, directional lighting, is something that this thesis does not explicitly tackle, but will demonstrate that the framework can be trivially extended to handle any form of lighting. Most photometric stereo methods are limited by assumption that light hits every patch at the same angle. This simplifies the optimization problem significantly since the solution is independent of the relative distance between the light and the mesh. (TODO: Near-light photometric stereo)

In practice, these assumptions can be ignored in a some cases, since grazing angle reflections don't account for much of the image, and based on the shape, interreflections may only contribute near sharp edges. In this thesis, we look at objects where we cannot ignore these assumptions. This includes either specular surfaces or highly concave objects, or both.

\section{Background}
The following section contains some background information that are relevant to the components of the framework presented in this thesis.
\subsection{Monte Carlo path tracing}
At the heart of any analysis-by-synthesis problem is the \textit{synthesis}. Since our framework aims to account for as many light transport phenomena as possible, it makes sense to use a fully-fledged phyically-based rendering method, like path tracing. \\
Path tracing is an umbrella term for a family of Monte Carlo estimators that are used to sample paths of light through a scene. One could fill a book with the various sampling techniques, each of which better than the last. However, they all estimate the same fundamental recursive equation known as the \textit{rendering equation}, one of the keystones of computer graphics.
(TODO: rendering equation)

For the purpose of this thesis, we use the popular algorithm BDPT, which represents a sweet spot in the trade-off between complexity and sample efficiency. 
At each iteration, the BDPT algorithm randomly sample two paths $\mathcal{L}$ and $\mathcal{C}$ from the light source and camera respectively of lengths $S$ and $T$ (which are determined by a Russian roulette factor). By connecting every point in $\mathcal{L}$ to every point in $\mathcal{C}$, BDPT creates a set of paths, along with the probability of path $s,t$ denoted by $p(\{\mathcal{L}_s,\mathcal{C}_t\})$.
The final intensity is then estimated by the equation:
(TODO: Place path integral here).

Note that for our framework, we only require that the integral be written as an integral over a set of sampled paths and their probabilities. This means we do not necessarily require BDPT. Any similar algorithm, like MLT or PSSMLT would also work once they generate a set of paths.

\subsection{Surface integration}
Surface integration is an key part of our framework, but given the extensive literature on the topic of gradients-to-depth algorithms, there is no need to come up with our own. Since we work with fairly noisy normals (an unfortunate side effect of stochastic gradient descent algorithms), a weighted integration algorithm like Poisson surface reconstruction is a perfect fit for our framework.

Weighted Poisson surface reconstruction can plainly expressed as the linear least squares solution to the following set of constraints.
(TODO: Add constraints)

\subsection{Adaptive Gradient Descent}
Gradient descent is a fundamental part of the non-linear optimizaton problem that is responsible for inferring the normals as well as BSDF parameters.
(TODO: Add SGD rules)

The need for Adaptive gradient descent algorithms stems from the fact that some parameters of a model can have a more drastic effect on the error than others, while also operating on the same scale (and range of values). This discrepancy introduces a catch-22 situation where a learning rate that is too small will take forever to converge and a learning rate that is too large will cause some of the parameters to diverge quickly (or behave unpredictably). This necessitates adaptive gradients, which take the form of many popular algorithms like AdaGrad, AdaDelta, Adam and RMSProp. We choose Adam since it is also the algorithm of choice for many neural networks, and has provably good characteristics.

(TODO: Add Adam rules)

Suppose you find yourself in possession of
\smallskip
\begin{itemize}
\item a calculator of unfamiliar design, or 
\item a new board game, or
\item the control system for an army of robots, or
\item an implementation of a security protocol, or
\item the interface to a high-frequency trading system.
\end{itemize}
\smallskip The fundamental questions are the same: {\it What does it
  do? What are the rules of the game?} The answer to this question,
whether it comes in the form of an instruction manual, a legal
document, or an ISO standard, is a {\it specification}.

Specifications must be {\it formal}, because any room for
misinterpretation could (respectively) lead to incorrect calculations,
accusations of cheating, a robot uprising, a security breach, or
bankruptcy. At the same time, specifications must be {\it clear}:
while clarity is in the eye of the beholder, a specification
that one finds hopelessly confusing or complex is no more useful than
one that is hopelessly vague.
%
Clarity is what allows us to
communicate with each other, to use specifications to gain a common
understanding of what some system does and to think about how that
system might be changed. Formality is what allows specifications to
interact with the world of computers, to say with confidence that the
{\it implementation} of the calculator or high-frequency trading
system obeys the specification. Formality also allows specifications
to interact with the world of mathematics, and this, in turn, enables
us to make precise and accurate statements about what may or may not
happen to a given system.

The specification of many (too many!)~critical systems still remains
in the realm of English text, and the inevitable lack of formality can
and does make formal reasoning about these specifications difficult or
impossible.
%
Notably, this is true about most of the programming languages used to
implement our calculators, program our robot army control systems,
enforce our security protocols, and interact with our high-frequency
trading systems. In the last decade, however, we have finally begun to
see the emergence of operational semantics specifications (the ``rules
of the game'' for a programming language) for real-world programming
languages that are truly formal.
%
A notable aspect of this recent work is that the formalization
effort is not done simply for formalization's sake. Ellison and Ro{\c
  s}u's formal semantics of C can be used to check individual programs
for undefined behavior, unsafe situations where the rules of the game
no longer apply and the compiler is free to do anything, including
unleashing the robot army \cite{ellison12executable}. Lee, Crary, and
Harper's formalization of Standard ML has been used to formally prove
-- using a computer to check all the proof's formal details -- a much
stronger safety property: that {\it every} program accepted by the
compiler is free of undefined behavior \cite{lee07towards}.

Mathematics, by contrast, has a century-long tradition of insisting on
absolute formality (at least in principle: practice often falls far
short).
%
Over time, this tradition has become a collaboration between
practicing mathematicians and practicing computer scientists, because
while humans are reasonable judges of clarity, computers have
absolutely superhuman patience when it comes to checking all the
formal details of an argument.
%
One aspect of
this collaboration has been the development of {\it logical
  frameworks}. In a logical framework, the language of specifications
is derived from the language of logic, which gives specifications in a
logical framework an independent meaning based on the logic from which
the logical framework was derived. To be clear, the language of logic
is not a single, unified entity: logics are formal systems that
satisfy certain internal coherence properties, and we study many of
them. For example, the logical framework Coq is based on the Calculus
of Inductive Constructions \cite{coq10coq}, the logical framework Agda
is based on a variant of Martin-L\"of's type theory called ${\sf
  UTT}_\Sigma$ \cite{norell08towards}, and the logical framework
Twelf is based
on the dependent type theory $\lambda^\Pi$, also known as LF
\cite{pfenning99system}. Twelf was the basis of Lee, Crary, and
Harper's formalization of Standard ML.  % Specifications evolve
% gradually from half-baked ideas scrawled on coffee-stained napkins to
% formal specifications encoded in a logical framework. Another critical
% component of a logical framework is a methodology or philosophy that
% guides this process. LF and Twelf, in particular, have a formal theory
% of {\it adequacy} that addresses the relationship between the on-paper
% artifacts the people use to communicate with each other and the
% encoding of those artifacts in LF \cite{harper93framework}.

Why is there not a larger tradition of formally specifying the
programming languages that people actually use? Part of the answer is
that most languages that people actually use have lots of features --
like mutable state, or exception handling, or synchronization and
communication, or lazy evaluation -- that are not particularly
pleasant to specify using existing logical frameworks. Dealing with a
few unpleasant features at a time might not be much trouble, but the
combinations that appear in actual programming languages cause formal
programming language specifications to be both unclear for humans to
read and inconvenient for formal tools to manipulate. A more precise
statement is that the addition of the aforementioned features is {\it
  non-modular}, because handling a new feature requires reconsidering
and revising the rest of the specification.  Some headway on this
problem has been made by frameworks like the K semantic framework that
are formal but not logically derived; the K semantic framework is
based on a formal system 
of rewriting rules \cite{rosu10overview}. Ellison
and Ro\c{s}u's formalization of C was done in the K semantic
framework.

This dissertation considers the specification of systems, particularly
programming languages, in logical frameworks. We
consider a particular family of logics, called {\it substructural
  logics}, in which logical propositions can be given an
interpretation as rewriting rules as detailed by Cervesato and Scedrov
\cite{cervesato09relating}. % Deriving a framework from substructural
% logics allows us to combine the formality and generality of logical
% frameworks with the modular specification that is possible in
% rewriting frameworks. With this synthesis, 
We seek to support the
following:
\smallskip
\begin{quote} {\bf Thesis Statement:} {\it Logical frameworks based on
    a rewriting interpretation of substructural logics are suitable
    for modular specification of programming languages and formal
    reasoning about their properties.}\footnote{The original thesis
    proposal used the phrase ``forward reasoning in substructural
    logics'' instead of the phrase ``a rewriting interpretation of
    substructural logics,'' but these are synonymous, as discussed in
    Section~\ref{sec:framework-logicprog}.}
\end{quote}
\smallskip

\noindent
Part~I of the dissertation covers the design of logical frameworks 
that support this rewriting interpretation and the design of the logical
framework \sls~in particular. Part~II considers the modular
specification of programming language features in \sls~and the
methodology by which we organize and relate styles of
specification. Part~III discusses formal reasoning about properties of
\sls~specifications, with an emphasis on establishing invariants.

\section{Logical frameworks}

Many interesting stateful systems have a natural notion of {\it
  ordering} that is fundamental to their behavior. A very simple
example is a push-down automaton (PDA) that reads a string of symbols
left-to-right while maintaining and manipulating a separate stack of
symbols. We can represent a PDA's internal configuration as a sequence
with three regions:
\[
[~\mbox{the stack}~]
~
[~\mbox{the head}~]
~
[~\mbox{the string being read}~]
\]
where the symbols closest to the head are the top of the stack and the
symbol waiting to be read from the string. If we represent the head as
a token ${\sf hd}$, we can describe the behavior (the rules of the
game) for the PDA that checks a string for correct
nesting of angle braces by using two rewriting rules:
\begin{align}
\tag{push} {\sf hd}~{<} ~&\rightsquigarrow~ {<}~{\sf hd}
\\
\tag{pop} {<}~{\sf hd}~{>} ~&\rightsquigarrow~ {\sf hd}
\end{align}
The distinguishing feature of these rewriting rules is that they are
{\it local} -- they do not mention the entire stack or the entire
string, just the relevant fragment at the beginning of the string and
the top of the stack. Execution of the PDA on a particular string of
tokens then consists of (1) appending the token ${\sf hd}$ to the
beginning of the string, (2) repeatedly performing rewritings until no
more rewrites are possible, and (3) checking to see if only a single
token ${\sf hd}$ remains. One possible series of transitions that this
rewriting system can take is shown in Figure~\ref{fig:pda-transitions}

\begin{figure}
\begin{align*}
{\sf hd}~~{<}~~{<}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{<}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{>}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{<}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{<}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{<}~~{\sf hd}~~{>}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{<}~~{\sf hd}~~{>}~~{>}
& ~~~\rightsquigarrow~~\\
{<}~~{\sf hd}~~{>}
& ~~~\rightsquigarrow~~\\
{\sf hd} &
\end{align*}
\caption{Series of PDA transitions}
\label{fig:pda-transitions}
\end{figure}

Because our goal is to use a framework that is both simple and
logically motivated, we turn to a substructural logic called {\it
  ordered logic}, a fragment of which was originally proposed by
Lambek for applications in computational
linguistics \cite{lambek58mathematics}. In ordered logic, hypotheses
are ordered relative to one another and
cannot be rearranged. The rewriting rules we considered
above can be expressed as propositions in ordered logic, where the
tokens ${\sf hd}$, $>$, and $<$ are all treated as {\it atomic
  propositions}:
\begin{align*}
{\sf push} &: ~~ {\sf hd} \fuse {<} ~\lefti~ \{ {<} \fuse {\sf hd} \}
\\ 
{\sf pop} &: ~~ {<} \fuse {\sf hd} \fuse {>} ~\lefti~ \{ {\sf hd} \}
\end{align*}
The symbol $\fuse$ (pronounced ``fuse'') is the binary connective for
ordered conjunction (i.e. concatenation); it binds more tightly than
$\lefti$, a binary connective for ordered implication. The curly
braces $\{ \ldots \}$ can be ignored for now.

The propositional fragment of ordered logic is
Turing complete: it is in fact
a simple exercise to specify a Turing machine!
Nevertheless, first-order quantification helps us write
specifications that are short and clear. For example, by using
first-order quantification we can describe a 
a more general push-down automaton in a generic way. In
this generic specification, we use ${\sf left}(X)$
and ${\sf right}(X)$ to describe left and right angle braces ($X =
{\sf an}$), square braces ($X = {\sf sq}$), and parentheses ($X = {\sf
  pa}$). The string \obj{\mbox{{\sf [ \textless~\textgreater~( [ ] ) ]}}} 
is then
represented by the following sequence of ordered atomic propositions:
\[
{\sf 
  left(sq) ~~
  left(an) ~~
  right(an) ~~
  left(pa) ~~
  left(sq) ~~
  right(sq) ~~
  right(pa) ~~
  right(sq)
}
\]
The following rules describe the more general push-down automaton:
\begin{align*}
{\sf push} &: ~~ \forall x.\, 
  {\sf hd} \fuse {\sf left}(x) ~\lefti~ \{ {\sf stack}(x) \fuse {\sf hd} \}
\\ 
{\sf pop} &: ~~ \forall x.\, 
  {\sf stack}(x) \fuse {\sf hd} \fuse {\sf right}(x) ~\lefti~ \{ {\sf hd} \}
\end{align*}
(This specification would still be possible in propositional ordered 
logic; we would just need one copy of the ${\sf push}$ rule and one copy
of the ${\sf pop}$ rule for each pair of braces.)
Note that while we use the fuse connective to indicate adjacent tokens
in the rules above, no fuses appear in
Figure~\ref{fig:pda-transitions}. That is because the intermediate
states are not propositions in the same way rules are
propositions. Rather, the intermediate states in
Figure~\ref{fig:pda-transitions} are {\it contexts} in ordered logic,
which we will refer to as {\it process states}. 

The most distinctive characteristic of these transition systems is
that the intermediate stages of computation are encoded in the
structure of a substructural context (a process state). This general
idea dates back to Miller \cite{miller92pi} and his Ph.D. student
Chirimar \cite{chirimar95proof}, who encoded the intermediate states
of a $\pi$-calculus and of a low-level RISC machine (respectively) as
contexts in focused classical linear logic.  Part~I of this dissertation
is
concerned with the design of logical frameworks for specifying
transition systems.  In this respect, Part~I follows
in the footsteps of Miller's Forum \cite{miller96forum}, Cervesato and
Scedrov's multiset rewriting language $\omega$
\cite{cervesato09relating}, and Watkins et al.'s CLF
\cite{watkins02concurrent}. 

\input{figs/fig-canonical}

As an extension to CLF, the logical framework we develop is able to
specify systems like the $\pi$-calculus, security protocols, and Petri
nets that can be encoded in CLF \cite{cervesato02concurrent}. The
addition of ordered logic allows us to easily incorporate
specifications that are naturally expressed as string rewriting
systems.
%
An example from
the verification domain, taken from Bouajjani and Esparza
\cite{bouajjani06rewriting}, is shown in Figure~\ref{fig:canonical}.
The left-hand side of the figure is a simple Boolean program: the
procedure ${\it foo}$ has one local variable and the procedure ${\it
  main}$ has no local variables but mentions a global variable $b$.
Bouajjani and Esparza represented Boolean programs like this one as
{\it canonical systems} like the one shown in the middle of
Figure~\ref{fig:canonical}. Canonical systems are rewriting systems
where only the left-most tokens are ever rewritten: the left-most
token in this canonical system always has the form $\langle b \rangle$,
where $b$ is either true (${\sf tt}$) or false (${\sf ff}$),
representing the valuation of the global variables -- there is only 
one, $b$.  The token to the
right of the global variables contains the current program counter and
the value of the current local variables. The token to the right of
{\it that} contains the program counter and local variables of the calling
procedure, and so on, forming a call stack that grows off to the right
(in contrast to the PDA's stack, which grew off to the left). Canonical
systems can be directly represented in ordered logic, as
shown on the right-hand side of Figure~\ref{fig:canonical}. The atomic
proposition ${\sf gl}(b)$ contains the global variables (versus
$\langle b \rangle$ in the middle column), the atomic proposition
${\sf foo}(l, f)$ contains the local variables and program counter
within the procedure ${\sf foo}$ (versus $\langle l, f \rangle$ in the
middle column), and the atomic proposition ${\sf main}(m)$ contains
the program counter within the procedure ${\sf main}$ (versus $\langle
m \rangle$ in the middle column).

The development of \sls, a CLF-like framework of \underline{\bf
  s}ubstructural \underline{\bf l}ogical \underline{\bf
  s}pecifications that includes an intrinsic notion of order, is a
significant development of Part~I of the dissertation.  However, the
principal contribution of these three chapters is the development of
{\it structural focalization}, which
unifies Andreoli's work on focused logics \cite{andreoli92logic} with
the {\it hereditary substitution} technique that Watkins developed in
the context of CLF
\cite{watkins02concurrent}. Chapter~\ref{chapter-foc} explains
structural focalization in the context of linear logic,
Chapter~\ref{chapter-order} establishes focalization for a richer
substructural logic \ollll, and Chapter~\ref{chapter-framework} takes
focused \ollll~and carves out the \sls~framework as a fragment of the
focused logic.

\section{Substructural operational semantics}
\label{sec:intro-ssos}

Existing logical frameworks are perfectly capable of representing
simple systems like PDAs, and while applications in the verification
domain like the rewriting semantics of Boolean programs are an
interesting application of \sls, they will not be a focus of this
dissertation. Instead, in Part~II, we will concentrate on
specifying the operational semantics of programming languages in \sls.
%
We can represent operational semantics in
\sls~in many ways, but we are particularly interested in a broad
specification style called {\it substructural operational semantics},
or SSOS
\cite{pfenning04substructural,pfenning09substructural}.\footnote{The
  term {\it substructural operational semantics} merges structural
  operational semantics \cite{plotkin04structural}, which we seek to
  generalize, and substructural logic, which forms the basis of our
  specification framework.} SSOS is a synthesis of structural
operational semantics, abstract machines, and logical specifications.

One of our running
examples will be a call-by-value operational semantics for the untyped
lambda calculus, defined by the BNF grammar:
\[
\obj{e} ::= \obj{x} \mid \obj{\lambda x.e} \mid \obj{e_1\,e_2}
\]
Taking some liberties with our representation of terms,\footnote{In
particular, we are leaving the first-order quantifiers implicit in this
section and using an informal {\it object language} representation
of syntax. The actual representation of syntax uses LF terms
that adequately encode this object language, as discussed in 
Section~\ref{sec:lf-adequacy}.} we can
describe call-by-value evaluation for this language with the same
rewriting rules we used to describe the PDA and the Boolean program's
semantics. Our specification uses three atomic propositions: one,
${\sf eval}(\obj{e})$, carries an unevaluated expression $\obj{e}$,
and another, ${\sf retn}(\obj{v})$, carries an evaluated value
$\obj{v}$.  The third atomic proposition, ${\sf cont}(\obj{f})$,
contains a {\it continuation frame} $\obj{f}$ that represents some
partially evaluated value: $\obj{f} = \obj{\Box\,e_2}$ contains an
expression $\obj{e_2}$ waiting on the evaluation of $\obj{e_1}$
to a value, and $\obj{f} = \obj{(\lambda x.e)\,\Box}$ contains
an function $\obj{\lambda x.e}$ waiting on the evaluation of
$\obj{e_2}$ to a value. These frames are arranged in a
stack that grows off to the right (like the Boolean program's stack).


The evaluation of a function is simple, as a function is already a
fully evaluated value, so we replace ${\sf eval}(\obj{\lambda x.e})$
in-place with ${\sf retn}(\obj{\lambda x.e})$:
\begin{align*}
{\sf ev/lam}&: ~~ 
  {\sf eval}\,(\obj{\lambda x.e})
      \lefti \{ {\sf retn}\,(\obj{\lambda x.e}) \}
%
      \intertext{The evaluation of an application $\obj{e_1\,e_2}$, on
        the other hand, requires us to push a new element onto the
        stack. We evaluate $\obj{e_1\,e_2}$ by evaluating $\obj{e_1}$
        and leaving behind a frame $\obj{\Box\,e_2}$ that suspends the
        argument $\obj{e_2}$ while $\obj{e_1}$ is being evaluated to a
        value.}
%
{\sf ev/app}&: ~~ 
  {\sf eval}\,(\obj{e_1\,e_2}) \lefti \{ {\sf eval}\,(\obj{e_1}) 
     \fuse {\sf cont}\,(\obj{\Box\,e_2}) \}
%
     \intertext{When a function is returned to a waiting $\obj{\Box\,e_2}$
       frame, we switch to evaluating the function argument while
       storing the returned function in a frame $\obj{(\lambda
       x.e)\,\Box}$.}
%
{\sf ev/app1}&: ~~
  {\sf retn}\,(\obj{\lambda x.e}) \fuse {\sf cont}\,(\obj{\Box\,e_2})
    \lefti \{ {\sf eval}\,(\obj{e_2})
      \fuse {\sf cont}\,(\obj{(\lambda x.e)\,\Box}) \}
%
    \intertext{Finally, when an evaluated function argument is
      returned to the waiting $\obj{(\lambda x.e)\,\Box}$ frame, we
      substitute the value into the body of the function and evaluate
      the result.}
%
{\sf ev/app2}&: ~~
  {\sf retn}\,(\obj{v_2}) \fuse {\sf cont}\,(\obj{(\lambda x.e)\,\Box})
    \lefti \{ {\sf eval}\,(\obj{[\obj{v_2}/\obj{x}]\obj{e}}) \}
%
\end{align*}

\begin{figure}
\begin{align*}
{\sf eval}\,(\obj{(\lambda x.x)\,((\lambda y.y)\,(\lambda z.e))}) 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf eval}\,(\obj{\lambda x.x}) \quad
{\sf cont}\,(\obj{\Box\,((\lambda y.y)\,(\lambda z.e))})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\obj{\lambda x.x}) \quad
{\sf cont}\,(\obj{\Box\,((\lambda y.y)\,(\lambda z.e))})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf eval}\,(\obj{(\lambda y.y)\,(\lambda z.e)}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf eval}\,(\obj{\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,(\lambda z.e)}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\obj{\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,(\lambda z.e)}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf eval}\,(\obj{\lambda z.e}) \quad
{\sf cont}\,(\obj{(\lambda y.y)\,\Box}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\obj{\lambda z.e}) \quad
{\sf cont}\,(\obj{(\lambda y.y)\,\Box}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf eval}\,(\obj{\lambda z.e}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\obj{\lambda z.e}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf eval}\,(\obj{\lambda z.e}) 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf retn}\,(\obj{\lambda z.e}) 
& ~~~\not\rightsquigarrow~~ 
\end{align*}
\caption{SSOS evaluation of an expression to a value}
\label{fig:ssos-example}
\end{figure}

These four rules constitute an SSOS specification of call-by-value
evaluation; an example of evaluating the expression $\obj{(\lambda
x.x)\,((\lambda y.y)\,(\lambda z.e))}$ to a value under this
specification is given in Figure~\ref{fig:ssos-example}.  Again, each
intermediate state is represented by a process state or ordered
context.

The \sls~framework admits many styles of specification. The SSOS
specification above resides in the {\it concurrent} fragment of
\sls. (This rewriting-like fragment is called concurrent because
rewriting specifications are naturally concurrent -- we can just as
easily seed the process state with two propositions ${\sf eval}(\obj{e})$
and ${\sf eval}(\obj{e'})$ that will evaluate to values concurrently and
independently,
side-by-side in the process state.)  Specifications in the concurrent
fragment of \sls~can take many different forms, a point that we will
discuss further in Chapter~\ref{chapter-correspondence}.

On the other end of the spectrum, the {\it deductive} fragment of
\sls~supports the specification of inductive definitions by the same
methodology used to represent inductive definitions in LF
\cite{harper93framework}.  We can therefore use the deductive fragment
of \sls~to specify a big-step operational semantics for call-by-value
evaluation by inductively defining the judgment $\obj{e \Downarrow
  v}$, which expresses that the expression $\obj{e}$ evaluates to the
value $\obj{v}$. On paper, this big-step operational semantics is
expressed with two inference rules:
\[
\infer
{\obj{\lambda x.e \Downarrow \lambda x.e} \mathstrut}
{}
\qquad
\infer
{\obj{e_1\,e_2 \Downarrow v} \mathstrut}
{\obj{e_1 \Downarrow \lambda x.e}
 &
 \obj{e_2 \Downarrow v_2}
 &
 \obj{{[\obj{v_2}/\obj{x}]\obj{e_2}} \Downarrow v} \mathstrut}
\]
Big-step operational semantics specifications are compact and elegant,
but they are not particularly {\it modular}. As a (contrived but illustrative)
example, consider the addition of a incrementing counter $\obj{\sf
  count}$ to the language of expressions $\obj{e}$. The counter is
a piece of runtime state, and every time $\obj{\sf count}$ is evaluated,
our runtime must 
return the value of the counter and then increments the
counter.\footnote{To keep the language small, we can represent
  numerals $\obj{\underline{\obj{n}}}$ as Church numerals:
  $\obj{\underline{\obj{0}}} = \obj{(\lambda f. \lambda x. x)}$,
  $\obj{\underline{\obj{1}}} = \obj{(\lambda f. \lambda x. f x)}$,
  $\obj{\underline{\obj{2}}} = \obj{(\lambda f. \lambda x. f (f x))}$,
  and so on.  Then, $\obj{\underline{\obj{n}} + \obj{1}} =
  \obj{\lambda f. \lambda x. f e}$ if $\obj{\underline{\obj{n}}} =
  \obj{\lambda f. \lambda x. e}$.}  To extend the big-step operational
semantics with this new feature, we have to revise all the existing
rules so that they mention the running counter:
%
\[
\infer
{\obj{({\sf count}, \underline{n}) \Downarrow 
  (\underline{n}, \underline{n} +1)} \mathstrut}
{}
\qquad 
\infer
{\obj{(\lambda x.e, \underline{n}) \Downarrow (\lambda x.e, \underline{n})}
 \mathstrut}
{}
\]
\[
\infer
{\obj{(e_1\,e_2, \underline{n}) \Downarrow (v, \underline{n'})} \mathstrut}
{\obj{(e_1, \underline{n}) \Downarrow (\lambda x.e, \underline n_1)}
 &
 \obj{(e_2, \underline n_1) \Downarrow (v_2, \underline n_2)}
 &
 \obj{({[\obj{v_2}/\obj{x}]\obj{e_2}}, \underline n_2 ) \Downarrow (v, \underline{n'})} \mathstrut}
\]

\begin{figure}
\begin{align*}
{\sf store}\,\obj{\underline 5} \quad
{\sf eval}\,(\obj{((\lambda x.\lambda y.y)\,{\sf count})\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf store}\,\obj{\underline 5} \quad
{\sf eval}\,(\obj{(\lambda x.\lambda y.y)\,{\sf count}}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app}$}\\
{\sf store}\,\obj{\underline 5} \quad
{\sf eval}\,(\obj{\lambda x.\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\obj{\underline 5} \quad
{\sf retn}\,(\obj{\lambda x.\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app1}$}\\
{\sf store}\,\obj{\underline 5} \quad
{\sf eval}\,(\obj{\sf count}) \quad
{\sf cont}\,(\obj{(\lambda x.\lambda y.y)\,\Box}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/count}$}\\
{\sf store}\,\obj{\underline 6} \quad
{\sf retn}\,(\obj{\underline 5}) \quad
{\sf cont}\,(\obj{(\lambda x.\lambda y.y)\,\Box}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\obj{\underline 6} \quad
{\sf eval}\,(\obj{\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\obj{\underline 6} \quad
{\sf retn}\,(\obj{\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,{\sf count}})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\obj{\underline 6} \quad
{\sf eval}\,(\obj{{\sf count}}) \quad
{\sf cont}\,(\obj{(\lambda y.y)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/count}$}\\
{\sf store}\,\obj{\underline 7} \quad
{\sf retn}\,(\obj{\underline 6}) \quad
{\sf cont}\,(\obj{(\lambda y.y)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/app2}$}\\
{\sf store}\,\obj{\underline 7} \quad
{\sf eval}\,(\obj{\underline 6}) 
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf ev/lam}$}\\
{\sf store}\,\obj{\underline 7} \quad
{\sf retn}\,(\obj{\underline 6}) 
& ~~~\not\rightsquigarrow~~ 
\end{align*}
\caption{Evaluation with an imperative counter}
\label{fig:eval-ssos-ctr}
\end{figure}


The simple elegance of our big-step operational semantics has been
tarnished by the need to deal with state, and each new stateful
feature requires a similar revision.  In contrast, our SSOS
specification can tolerate the addition of a counter without revision
to the existing rules; we just store the counter's value in an atomic
proposition ${\sf store}(\obj{\underline{n}})$ to the left of the ${\sf
  eval}(\obj{e})$ or ${\sf retn}(\obj{v})$ proposition in the ordered
context. Because the rules ${\sf ev/lam}$, ${\sf ev/app}$, ${\sf
  ev/app1}$, and ${\sf ev/app2}$ are local, they will ignore this
extra proposition, which only needs to be accessed by the rule ${\sf
  ev/count}$.
\begin{align*}
{\sf ev/count} &:~~
  {\sf store}\,\obj{\underline n} \fuse {\sf eval}\,\obj{{\sf count}}
    \lefti \{ {\sf store}\,(\obj{\underline n + 1}) 
      \fuse {\sf retn}\,\obj{\underline n} \}
\end{align*}
In Figure~\ref{fig:eval-ssos-ctr}, we give an example of evaluating
$\obj{(((\lambda x.\lambda y.y)\,{\sf count})\,{\sf count})}$ to a
value with a starting counter value of $\obj{\underline 5}$. This
specific solution -- adding a counter proposition to the left of the
${\sf eval}$ or ${\sf retn}$ -- is rather contrived. We want, in
general, to be able to add arbitrary state, and this technique only
allows us to add {\it one} piece of runtime state easily: if we wanted
to introduce a {\it second} counter, where would it go? Nevertheless,
the example does foreshadow how, in Part~II of this dissertation, we will
show that SSOS specifications in \sls~allow for the modular
specification of many programming language features.


An overarching theme of Part~II is that we can have our cake and eat
it too by deploying the {\it logical correspondence}, an idea that was
developed jointly with Ian Zerny and that is explained in
Chapter~\ref{chapter-correspondence}. In
Chapter~\ref{chapter-absmachine}, we show how we can use the logical
correspondence to directly connect the big-step semantics and SSOS
specifications above; in fact, we can automatically and mechanically
derive the latter from the former. As our example above showed,
big-step operational semantics do not support combining the
specification of pure features (like call-by-value evaluation) with
the specification of a stateful feature (like the counter) -- or, at
least, doing so requires more than concatenating the specifications.
Using the automatic transformations described in
Chapter~\ref{chapter-absmachine}, we can specify pure features (like
call-by-value evaluation) as a simpler big-step semantics
specification, and then we can compose that specification with an SSOS
specification of stateful features (like the counter) by mechanically
transforming the big-step semantics part of the specification into
SSOS. In SSOS, the extension is modular: the call-by-value
specification can be extended by just adding new rules for the
counter.  Further transformations, developed in joint work with
Pfenning \cite{simmons11logical}, create new opportunities for modular
extension; this is the topic of Chapter~\ref{chapter-destinations}.

Appendix~\ref{appendix-hybrid} puts the logical correspondence to work
by demonstrating that we can create a single coherent language
specification by composing four different styles of specification.
Pure features are given a natural semantics, whereas stateful,
concurrent, and control features are specified at the most
``high-level'' SSOS specification style that is appropriate. The
automatic transformations that are the focus of Part~II then transform
the specifications into a single coherent specification.

Transformations on \sls~specifications also allow us to derive
abstract analyses (such as control flow and alias analysis) directly
from SSOS specifications. This methodology for program abstraction,
{\it linear logical approximation}, is the focus of
Chapter~\ref{chapter-approx}.

\section{Invariants in substructural logic}

A prominent theme in work on model checking and rewriting logic is
expressing invariants in terms of temporal logics like LTL and
verifying these properties with exhaustive state-space exploration
\cite[Chapter 10]{clavel11ltl}.
In Part~III of this dissertation
we offer an approach to invariants that is complementary to
this model checking approach.  From a programming languages
perspective, invariants are often associated with {\it types}. Type
invariants are well-formedness criteria on programs that are weak
enough to be preserved by state transitions (a property called {\it
  preservation}) but strong enough to allow us to express the properties
we expect to hold of all well-formed program states. In systems free
of deadlock, a common property we want to hold is {\it progress} -- a
well-typed state is either final or it can evolve to some other state with a
state transition. (Even in systems where deadlock is a possibility,
progress can be handled by stipulating that a deadlocked state is
final.) Progress and preservation together imply the safety property
that a language is free of unspecified behavior. 

Chapter~\ref{chapter-gen} discusses the use of {\it generative signatures} to
describe well-formedness invariants of specifications. Generative
signatures look like a generalization of 
context-free grammars, and they allow us to
characterize contexts by a describing rewriting rules that generate
legal or well-formed
process states in the same way that context-free grammars characterize
grammatical strings by describing rules that generate all grammatical
strings.

In our example SSOS specification, a process state that consists of
only a single ${\sf retn}(\obj{v})$ proposition is final, and a
well-formed state is any state that consists of an atomic proposition ${\sf
  eval}(\obj{e})$ (where $\obj{e}$ is a closed expression) or ${\sf
  retn}(\obj{\lambda x. e})$ (where $\obj{\lambda x. e}$ is a closed
expression) to the left 
of a series of continuation frames ${\sf cont}(\obj{\Box\,e})$ 
or ${\sf cont}(\obj{(\lambda x.e)\,\Box})$. We can characterize 
all such states as being generated from an initial atomic 
proposition ${\sf gen\_state}$ under the following generative
signature:
\begin{align*}
{\sf gen/eval} &:~~
  {\sf gen\_state} \lefti \{ {\sf eval}(\obj{e}) \}
\\
{\sf gen/retn} &:~~
  {\sf gen\_state} \lefti \{ {\sf retn}(\obj{\lambda x.e}) \}
\\
{\sf gen/app1} &:~~
  {\sf gen\_state} 
    \lefti \{ {\sf gen\_state} \fuse {\sf cont}(\obj{\Box\,e_2}) \}
\\
{\sf gen/app2} &:~~
  {\sf gen\_state} 
    \lefti \{ {\sf gen\_state} \fuse {\sf cont}(\obj{(\lambda x.e)\,\Box}) \}
\end{align*}
The derivation of one of the intermediate process
states from Figure~\ref{fig:ssos-example} is shown in
Figure~\ref{fig:ssos-gen}.

\begin{figure}
\begin{align*}
{\sf gen\_state}
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf gen/app2}$}\\
{\sf gen\_state} \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf gen/app1}$}\\
{\sf gen\_state} \quad
{\sf cont}\,(\obj{\Box\,(\lambda z.e)}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\rightsquigarrow~~ \tag{by rule ${\sf gen/retn}$}\\
{\sf retn}\,(\obj{\lambda y.y}) \quad
{\sf cont}\,(\obj{\Box\,(\lambda z.e)}) \quad
{\sf cont}\,(\obj{(\lambda x.x)\,\Box})
& ~~~\not\rightsquigarrow~~ 
\end{align*}
\caption{Proving well-formedness of one of the states from
  Figure~\ref{fig:ssos-example}}
\label{fig:ssos-gen}
\end{figure}

Well-formedness is a global property of specifications. Therefore, 
if we add state to the specification, we have to change the description of
what counts as a final state and extend the grammar of well-formed
process states. In the case of our counter extension, final states
have a single ${\sf store}(\obj{\underline n})$ proposition to the left of a
single ${\sf retn}(\obj{v})$ proposition, and well-formed states are
generated from an initial atomic proposition ${\sf gen}$ under the
following extension to the previous generative signature:
%
\begin{align*}
{\sf gen/all} &:~~ {\sf gen} \lefti \{ {\sf gen\_store} \fuse {\sf
  gen\_state} \}
\\
{\sf gen/store} &:~~ {\sf gen\_store} \lefti \{ {\sf
  store}(\obj{\underline n}) \}
\end{align*}

The grammar above describes a very coarse invariant of our SSOS
specification, and it is possible to prove that specifications
preserve more expressive invariants. An important class of examples
are invariants about the types of expressions and process states,
which will be considered in Chapter~\ref{chapter-gen}.  For almost any
SSOS specification more complicated than the one given above, type
invariants are necessary for proving the progress theorem and
concluding that the specification is safe -- that is, free from
undefined behavior.  Chapter~\ref{chapter-safety} will consider the
use of generative invariants for proving safety properties of
specifications.

\section{Contributions}

The three parts of this dissertation support three different
aspects of our central thesis, which we can state as refined thesis
statements that support the central thesis. We will presently discuss
these supporting thesis statements along with the major contributions
associated with each of the refinements.

\smallskip
\begin{quote} {\bf Thesis (Part I):} {\it The methodology of
    structural focalization facilitates the derivation of logical
    frameworks as fragments of focused logics.}
\end{quote}
\smallskip

\noindent
The first major contribution of Part~I of the dissertation is the
development of {\it structural focalization} and its application
to linear logic (Chapter~\ref{chapter-foc}) and ordered linear lax
logic (Chapter~\ref{chapter-order}). The second major contribution is
the justification of the logical framework \sls~as a fragment of a
focused logic, generalizing the {\it hereditary substitution}
methodology of Watkins \cite{watkins02concurrent}.

\smallskip
\begin{quote} 
  {\bf Thesis (Part II):} {\it A logical framework based on a rewriting
  interpretation of substructural logic supports many styles of
  programming language specification. These styles can be formally
  classified and connected by considering general transformations on
  logical specifications.}
\end{quote} 
\smallskip

\noindent
The major contribution of Part~II is the development of the {\it
  logical correspondence}, a methodology for extending, classifying,
inter-deriving, and modularly extending operational semantics
specifications that are encoded in \sls, with an emphasis on SSOS
specifications. The transformations in
Chapter~\ref{chapter-absmachine} connect big-step operational
semantics specifications and the ordered abstract machine-style SSOS
semantics that we introduced in Section~\ref{sec:intro-ssos}. The
destination-adding transformation given in
Chapter~\ref{chapter-destinations} connects these specifications with
the older {\it destination-passing style} of SSOS specification. In
both chapters the transformations we discuss 
add new opportunities for modular extension -- that is,
new opportunities to add features to the language specification without
revising existing rules. The transformations in these chapters are
implemented in the \sls~prototype, as demonstrated by the development
in Appendix~\ref{appendix-hybrid}.

\smallskip
\begin{quote} 
  {\bf Thesis (Part III):} {\it The \sls~specification of the operational
    semantics of a programming language is a suitable basis for formal
    reasoning about properties of the specified language.}
\end{quote} 
\smallskip 

\noindent
We discuss two techniques for formal reasoning about the properties of
SSOS specifications in \sls. In Chapter~\ref{chapter-approx} we
discuss the logical approximation methodology and show that it can be
used to take SSOS specifications and derive known control flow and
alias analyses that are correct by construction.  The use of
generative signatures to describe invariants is discussed in
Chapter~\ref{chapter-gen}, and the use of these invariants to prove
safety properties of programming languages is discussed in
Chapter~\ref{chapter-safety}. 
